{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Fused window process have not been installed. Please refer to get_started.md for installation.\n",
      "Tutel has not been installed. To use Swin-MoE, please install Tutel; otherwise, just ignore this.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as FT\n",
    "from torchvision import utils\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import subprocess\n",
    "# from generate_shapes_and_images import generate, generateImage\n",
    "from model import Generator\n",
    "\n",
    "# from myService.myModel import *\n",
    "# from myService.myDataset import MyDataset\n",
    "from myService.myUtils import my_collate\n",
    "from myService.getImages import GetImages\n",
    "from options import BaseOptions\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "from swinModels.swin_transformer import SwinTransformer\n",
    "\n",
    "random.seed(datetime.now().timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= 100\n",
    "batch_size= 64\n",
    "\n",
    "truncation_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--dataset_path DATASET_PATH]\n",
      "                             [--config CONFIG] [--expname EXPNAME]\n",
      "                             [--ckpt CKPT] [--continue_training]\n",
      "                             [--checkpoints_dir CHECKPOINTS_DIR] [--iter ITER]\n",
      "                             [--batch BATCH] [--chunk CHUNK]\n",
      "                             [--val_n_sample VAL_N_SAMPLE]\n",
      "                             [--d_reg_every D_REG_EVERY]\n",
      "                             [--g_reg_every G_REG_EVERY]\n",
      "                             [--local_rank LOCAL_RANK] [--mixing MIXING]\n",
      "                             [--lr LR] [--r1 R1] [--view_lambda VIEW_LAMBDA]\n",
      "                             [--eikonal_lambda EIKONAL_LAMBDA]\n",
      "                             [--min_surf_lambda MIN_SURF_LAMBDA]\n",
      "                             [--min_surf_beta MIN_SURF_BETA]\n",
      "                             [--path_regularize PATH_REGULARIZE]\n",
      "                             [--path_batch_shrink PATH_BATCH_SHRINK] [--wandb]\n",
      "                             [--no_sphere_init] [--results_dir RESULTS_DIR]\n",
      "                             [--truncation_ratio TRUNCATION_RATIO]\n",
      "                             [--truncation_mean TRUNCATION_MEAN]\n",
      "                             [--identities IDENTITIES]\n",
      "                             [--num_views_per_id NUM_VIEWS_PER_ID]\n",
      "                             [--no_surface_renderings] [--fixed_camera_angles]\n",
      "                             [--azim_video] [--size SIZE]\n",
      "                             [--style_dim STYLE_DIM]\n",
      "                             [--channel_multiplier CHANNEL_MULTIPLIER]\n",
      "                             [--n_mlp N_MLP] [--lr_mapping LR_MAPPING]\n",
      "                             [--renderer_spatial_output_dim RENDERER_SPATIAL_OUTPUT_DIM]\n",
      "                             [--project_noise] [--uniform] [--azim AZIM]\n",
      "                             [--elev ELEV] [--fov FOV]\n",
      "                             [--dist_radius DIST_RADIUS] [--depth DEPTH]\n",
      "                             [--width WIDTH] [--no_sdf] [--no_z_normalize]\n",
      "                             [--static_viewdirs] [--N_samples N_SAMPLES]\n",
      "                             [--no_offset_sampling] [--perturb PERTURB]\n",
      "                             [--raw_noise_std RAW_NOISE_STD]\n",
      "                             [--force_background] [--return_xyz]\n",
      "                             [--return_sdf]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=c:\\Users\\vince\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-199240E3iTH15JHvJ.json could match --fixed_camera_angles, --fov, --force_background\n"
     ]
    }
   ],
   "source": [
    "inference_identities = 1\n",
    "\n",
    "opt = BaseOptions().parse()\n",
    "opt.camera.uniform = True\n",
    "opt.model.is_test = True\n",
    "opt.model.freeze_renderer = False\n",
    "opt.rendering.offset_sampling = True\n",
    "opt.rendering.static_viewdirs = True\n",
    "opt.rendering.force_background = True\n",
    "opt.rendering.perturb = 0\n",
    "opt.inference.renderer_output_size = opt.model.renderer_spatial_output_dim = 64\n",
    "opt.inference.style_dim = opt.model.style_dim\n",
    "opt.inference.project_noise = opt.model.project_noise\n",
    "\n",
    "# User options\n",
    "model_type = 'ffhq' # Whether to load the FFHQ or AFHQ model\n",
    "opt.inference.no_surface_renderings = True # When true, only RGB images will be created\n",
    "opt.inference.fixed_camera_angles = False # When true, each identity will be rendered from a specific set of 13 viewpoints. Otherwise, random views are generated\n",
    "opt.inference.identities = inference_identities # Number of identities to generate\n",
    "opt.inference.num_views_per_id = 1 # Number of viewpoints generated per identity. This option is ignored if self.opt.inference.fixed_camera_angles is true.\n",
    "\n",
    "opt.model.size = 1024\n",
    "opt.experiment.expname = 'ffhq1024x1024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = 1\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() & use_cuda) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    '''\n",
    "    load the dataset\n",
    "    '''\n",
    "    def __init__(self, startIdx, endIdx, transform = None):\n",
    "        self.startIdx = startIdx\n",
    "        self.endIdx = endIdx\n",
    "        \n",
    "        camera_json_path = './prepareDataset/json/camera_paras.json'\n",
    "        # z_json_path = './prepareDataset/json/sample_z_actual_used.json'\n",
    "        z_json_path = './prepareDataset/json/sample_z.json'\n",
    "        \n",
    "        default_transform = transforms.Compose([\n",
    "            # transforms.Resize((28,28)),\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "        with open(camera_json_path) as jsonFile:\n",
    "            camera_paras = json.load(jsonFile)\n",
    "\n",
    "        with open(z_json_path) as jsonFile:\n",
    "            sample_z = json.load(jsonFile)\n",
    "        \n",
    "        self.camera_paras = camera_paras[startIdx: endIdx]\n",
    "        self.sample_z = sample_z[startIdx: endIdx]\n",
    "        # print(f'self.sample_z.shape: {np.array(self.sample_z).shape}')\n",
    "\n",
    "        if transform == None:\n",
    "            self.transform = default_transform\n",
    "        else:\n",
    "            self.transform = transform\n",
    "        print('number of total data:{}'.format(len(self.camera_paras)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.camera_paras)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        :param idx: Index of the image file\n",
    "        :return: returns the image and corresponding label file.\n",
    "        '''\n",
    "        # read image with PIL module\n",
    "        image_name = './prepareDataset/thumbs/' + str(self.startIdx + idx).rjust(7, \"0\") + \".png\"\n",
    "        image = Image.open(image_name, mode='r')\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # len: 256\n",
    "        sample_z = self.sample_z[idx][0] \n",
    "        # print(np.array(sample_z).shape)\n",
    "\n",
    "        # len: 12\n",
    "        # camera_paras = np.array(self.camera_paras[self.startIdx + idx][\"sample_cam_extrinsics\"][0]).flatten().tolist() + self.camera_paras[self.startIdx + idx][\"sample_locations\"][0]\n",
    "        camera_paras = np.array(self.camera_paras[idx][\"sample_cam_extrinsics\"][0]).flatten().tolist()\n",
    "\n",
    "        # len: 256 + 12 = 268\n",
    "        target = sample_z + camera_paras\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "        return (image, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of total data:8000\n",
      "number of total data:2000\n"
     ]
    }
   ],
   "source": [
    "train_test_split_point = 8000\n",
    "trainData = MyDataset(startIdx=0, endIdx=train_test_split_point, transform=None)\n",
    "trainData_loader = DataLoader(trainData, batch_size=batch_size, num_workers=0,  collate_fn = my_collate, shuffle=True)\n",
    "\n",
    "testData = MyDataset(startIdx=train_test_split_point, endIdx=10001, transform=None)\n",
    "testData_loader = DataLoader(testData, batch_size=batch_size, num_workers=0,  collate_fn = my_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 64, 64])\n",
      "torch.Size([64, 268])\n"
     ]
    }
   ],
   "source": [
    "for data in trainData_loader:\n",
    "    print(data[0].shape)\n",
    "    print(data[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvM(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        super(ConvM, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "            norm_layer(out_planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, n_class=16):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            ConvM(3, 32, 5, 2),\n",
    "            ConvM(32, 32, 5, 2),\n",
    "            ConvM(32, 32, 3, 1),\n",
    "            ConvM(32, 32, 3, 1),\n",
    "        )        \n",
    "        self.fc1 = nn.Linear(32, 256)\n",
    "        self.fc2 = nn.Linear(256, 1000)\n",
    "        self.fc3 = nn.Linear(1000, n_class)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, 1).reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block with instance normalization.\"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.main(x)\n",
    "class ResnetEncoder(nn.Module):\n",
    "    # 202005251539 attr dim\n",
    "    def __init__(self, input_nc=3, output_nc=3, n_blocks=3): \n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetEncoder, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        ngf = 64\n",
    "        padding_type ='reflect'\n",
    "        norm_layer = nn.InstanceNorm2d\n",
    "        use_bias = False\n",
    "        \n",
    "        model = [nn.Conv2d(input_nc , ngf, kernel_size=7, padding=3,\n",
    "                           bias=use_bias),\n",
    "                 norm_layer(ngf, affine=True, track_running_stats=True),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=4,\n",
    "                                stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2, affine=True, track_running_stats=True),\n",
    "                      nn.ReLU(True)]\n",
    "        mult = 2**n_downsampling\n",
    "        \n",
    "        for i in range(n_blocks):\n",
    "            model += [ResidualBlock(dim_in=ngf * mult, dim_out=ngf * mult)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        # 65536\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, 268)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "convNet = ConvNet(n_class=268).to(device)\n",
    "resnetEncoder = ResnetEncoder().to(device)\n",
    "swin_transformer = SwinTransformer(img_size=64, window_size=4, num_classes=268).to(device)\n",
    "\n",
    "encoder = resnetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model\n",
    "# epoch = 10\n",
    "# resnetEncoder_path = f\"./result/models/{epoch}.ckpt\"\n",
    "# resnetEncoder.load_state_dict(torch.load(resnetEncoder_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "lossModel_image = torch.nn.MSELoss().to(device)\n",
    "# lossModel_image = torch.nn.CrossEntropyLoss().to(device)\n",
    "# lossModel_image = torch.nn.L1Loss().to(device)\n",
    "lossModel_latent = torch.nn.MSELoss().to(device)\n",
    "# lossModel_latent = torch.nn.CrossEntropyLoss().to(device)\n",
    "# lossModel_latent = torch.nn.L1Loss().to(device)\n",
    "# loss = torch.nn.SmoothL1Loss().to(device)\n",
    "# optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=0.001, betas=[0.5, 0.999])\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20,30,40,50,60,70,80,90], gamma=0.7)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (style): Sequential(\n",
       "    (0): MappingLinear(256, 256)\n",
       "    (1): MappingLinear(256, 256)\n",
       "    (2): MappingLinear(256, 256)\n",
       "  )\n",
       "  (renderer): VolumeFeatureRenderer(\n",
       "    (network): SirenGenerator(\n",
       "      (pts_linears): ModuleList(\n",
       "        (0): FiLMSiren(\n",
       "          (gamma): LinearLayer()\n",
       "          (beta): LinearLayer()\n",
       "        )\n",
       "        (1): FiLMSiren(\n",
       "          (gamma): LinearLayer()\n",
       "          (beta): LinearLayer()\n",
       "        )\n",
       "        (2): FiLMSiren(\n",
       "          (gamma): LinearLayer()\n",
       "          (beta): LinearLayer()\n",
       "        )\n",
       "        (3): FiLMSiren(\n",
       "          (gamma): LinearLayer()\n",
       "          (beta): LinearLayer()\n",
       "        )\n",
       "        (4): FiLMSiren(\n",
       "          (gamma): LinearLayer()\n",
       "          (beta): LinearLayer()\n",
       "        )\n",
       "        (5): FiLMSiren(\n",
       "          (gamma): LinearLayer()\n",
       "          (beta): LinearLayer()\n",
       "        )\n",
       "        (6): FiLMSiren(\n",
       "          (gamma): LinearLayer()\n",
       "          (beta): LinearLayer()\n",
       "        )\n",
       "        (7): FiLMSiren(\n",
       "          (gamma): LinearLayer()\n",
       "          (beta): LinearLayer()\n",
       "        )\n",
       "      )\n",
       "      (views_linears): FiLMSiren(\n",
       "        (gamma): LinearLayer()\n",
       "        (beta): LinearLayer()\n",
       "      )\n",
       "      (rgb_linear): LinearLayer()\n",
       "      (sigma_linear): LinearLayer()\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (style): Sequential(\n",
       "      (0): PixelNorm()\n",
       "      (1): EqualLinear(256, 512)\n",
       "      (2): EqualLinear(512, 512)\n",
       "      (3): EqualLinear(512, 512)\n",
       "      (4): EqualLinear(512, 512)\n",
       "      (5): EqualLinear(512, 512)\n",
       "    )\n",
       "    (conv1): StyledConv(\n",
       "      (conv): ModulatedConv2d(256, 512, 3, upsample=False, downsample=False)\n",
       "      (noise): NoiseInjection()\n",
       "      (activate): FusedLeakyReLU()\n",
       "    )\n",
       "    (to_rgb1): ToRGB(\n",
       "      (conv): ModulatedConv2d(512, 3, 1, upsample=False, downsample=False)\n",
       "    )\n",
       "    (convs): ModuleList(\n",
       "      (0): StyledConv(\n",
       "        (conv): ModulatedConv2d(512, 256, 3, upsample=True, downsample=False)\n",
       "        (noise): NoiseInjection()\n",
       "        (activate): FusedLeakyReLU()\n",
       "      )\n",
       "      (1): StyledConv(\n",
       "        (conv): ModulatedConv2d(256, 256, 3, upsample=False, downsample=False)\n",
       "        (noise): NoiseInjection()\n",
       "        (activate): FusedLeakyReLU()\n",
       "      )\n",
       "      (2): StyledConv(\n",
       "        (conv): ModulatedConv2d(256, 128, 3, upsample=True, downsample=False)\n",
       "        (noise): NoiseInjection()\n",
       "        (activate): FusedLeakyReLU()\n",
       "      )\n",
       "      (3): StyledConv(\n",
       "        (conv): ModulatedConv2d(128, 128, 3, upsample=False, downsample=False)\n",
       "        (noise): NoiseInjection()\n",
       "        (activate): FusedLeakyReLU()\n",
       "      )\n",
       "      (4): StyledConv(\n",
       "        (conv): ModulatedConv2d(128, 64, 3, upsample=True, downsample=False)\n",
       "        (noise): NoiseInjection()\n",
       "        (activate): FusedLeakyReLU()\n",
       "      )\n",
       "      (5): StyledConv(\n",
       "        (conv): ModulatedConv2d(64, 64, 3, upsample=False, downsample=False)\n",
       "        (noise): NoiseInjection()\n",
       "        (activate): FusedLeakyReLU()\n",
       "      )\n",
       "      (6): StyledConv(\n",
       "        (conv): ModulatedConv2d(64, 32, 3, upsample=True, downsample=False)\n",
       "        (noise): NoiseInjection()\n",
       "        (activate): FusedLeakyReLU()\n",
       "      )\n",
       "      (7): StyledConv(\n",
       "        (conv): ModulatedConv2d(32, 32, 3, upsample=False, downsample=False)\n",
       "        (noise): NoiseInjection()\n",
       "        (activate): FusedLeakyReLU()\n",
       "      )\n",
       "    )\n",
       "    (upsamples): ModuleList()\n",
       "    (to_rgbs): ModuleList(\n",
       "      (0): ToRGB(\n",
       "        (upsample): Upsample()\n",
       "        (conv): ModulatedConv2d(256, 3, 1, upsample=False, downsample=False)\n",
       "      )\n",
       "      (1): ToRGB(\n",
       "        (upsample): Upsample()\n",
       "        (conv): ModulatedConv2d(128, 3, 1, upsample=False, downsample=False)\n",
       "      )\n",
       "      (2): ToRGB(\n",
       "        (upsample): Upsample()\n",
       "        (conv): ModulatedConv2d(64, 3, 1, upsample=False, downsample=False)\n",
       "      )\n",
       "      (3): ToRGB(\n",
       "        (upsample): Upsample()\n",
       "        (conv): ModulatedConv2d(32, 3, 1, upsample=False, downsample=False)\n",
       "      )\n",
       "    )\n",
       "    (noises): Module()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get g_ema\n",
    "model_path = 'ffhq1024x1024.pt'\n",
    "checkpoint_path = os.path.join('full_models', model_path)\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "g_ema = Generator(model_opt=opt.model, renderer_opt=opt.rendering, full_pipeline=True).to(device)\n",
    "\n",
    "pretrained_weights_dict = checkpoint[\"g_ema\"]\n",
    "# pretrained_weights_dict = checkpoint[\"g\"]\n",
    "model_dict = g_ema.state_dict()\n",
    "for k, v in pretrained_weights_dict.items():\n",
    "    if v.size() == model_dict[k].size():\n",
    "        model_dict[k] = v\n",
    "\n",
    "g_ema.load_state_dict(model_dict)\n",
    "g_ema.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prepareDataset/json/mean_latent.json\", 'r') as jsonFile:\n",
    "    mean_latent = json.load(jsonFile)\n",
    "for i in range(len(mean_latent)):\n",
    "    mean_latent[i] = torch.Tensor(mean_latent[i]).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateImage(latent, g_ema, seeIdx, fileName, save=True):\n",
    "    # all are tensor\n",
    "    latent = latent[seeIdx]\n",
    "    \n",
    "    sample_z = latent[0:256].reshape(1,256)\n",
    "    sample_cam_extrinsics = torch.Tensor([[\n",
    "        latent[256:260].tolist(),\n",
    "        latent[260:264].tolist(),\n",
    "        latent[264:268].tolist(),\n",
    "    ]]).to(device)\n",
    "    sample_focals = torch.Tensor([[\n",
    "        [\n",
    "            304.45965576171875\n",
    "        ]\n",
    "    ]]).to(device)\n",
    "    sample_near = torch.Tensor([[\n",
    "        [\n",
    "            0.8799999952316284\n",
    "        ]\n",
    "    ]]).to(device)\n",
    "    sample_far = torch.Tensor([[\n",
    "        [\n",
    "            1.1200000047683716\n",
    "        ]\n",
    "    ]]).to(device)\n",
    "    out = g_ema([sample_z],\n",
    "            sample_cam_extrinsics,\n",
    "            sample_focals,\n",
    "            sample_near,\n",
    "            sample_far,\n",
    "            truncation=truncation_ratio,\n",
    "            truncation_latent=mean_latent)\n",
    "    opt.renderer_output_size = 64\n",
    "    rgb_images_thumbs = torch.Tensor(0, 3, opt.renderer_output_size, opt.renderer_output_size)\n",
    "    rgb_images_thumbs = torch.cat([rgb_images_thumbs, out[1].cpu()], 0)\n",
    "\n",
    "    rgb_images_thumbs = rgb_images_thumbs.reshape(3,64,64)\n",
    "    # print(f'generate: {rgb_images_thumbs}')\n",
    "    if save:\n",
    "        utils.save_image(rgb_images_thumbs,\n",
    "            # os.path.join(prepareDatasetPath, 'thumbs',f'{str(i).zfill(7)}.png'),\n",
    "            f'./result/images/{fileName}.png',\n",
    "            nrow=1,\n",
    "            normalize=True,\n",
    "            padding=0,\n",
    "            value_range=(-1, 1),)\n",
    "    # save_image([rgb_images_thumbs], f'./result/images/{fileName}.png')\n",
    "    \n",
    "    # image = Image.fromarray(rgb_images_thumbs.permute(1, 2, 0))\n",
    "    # image.save(f'{fileName}.png', format='PNG')\n",
    "    # %matplotlib inline\n",
    "    # plt.imshow(  rgb_images_thumbs.permute(1, 2, 0)  )\n",
    "    # plt.imshow(  rgb_images_thumbs  )\n",
    "    return rgb_images_thumbs\n",
    "\n",
    "def generateImageBatch(latent, para, g_ema):\n",
    "    sample_cam_extrinsics = para[:, :12].reshape(-1,3,4)\n",
    "    sample_focals = torch.Tensor([[\n",
    "        [\n",
    "            304.45965576171875\n",
    "        ]\n",
    "    ]]*batch_size).to(device)\n",
    "    sample_near = torch.Tensor([[\n",
    "        [\n",
    "            0.8799999952316284\n",
    "        ]\n",
    "    ]]*batch_size).to(device)\n",
    "    sample_far = torch.Tensor([[\n",
    "        [\n",
    "            1.1200000047683716\n",
    "        ]\n",
    "    ]]*batch_size).to(device)\n",
    "    \n",
    "    chunk = 2\n",
    "    \n",
    "    thumb_rgb = torch.Tensor(0, 3, 64, 64).to(device)\n",
    "    thumb_rgb.requires_grad = True\n",
    "\n",
    "    for j in range(0, batch_size, chunk):\n",
    "        out = g_ema([latent[j:j+chunk]],\n",
    "                    sample_cam_extrinsics[j:j+chunk],\n",
    "                    sample_focals[j:j+chunk],\n",
    "                    sample_near[j:j+chunk],\n",
    "                    sample_far[j:j+chunk],\n",
    "                    truncation=truncation_ratio,\n",
    "                    truncation_latent=mean_latent)\n",
    "\n",
    "        # rgb_images = torch.cat([rgb_images, out[0].cpu()], 0)\n",
    "        thumb_rgb = torch.cat([thumb_rgb, out[1]], 0)\n",
    "\n",
    "        del out\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return thumb_rgb\n",
    "    \n",
    "    # all are tensor\n",
    "    # rgb_images_thumbs_list = []\n",
    "    # for i in range(len(latent)):\n",
    "    #     rgb_images_thumbs = generateImage(latent=latent, g_ema=g_ema, seeIdx=i, fileName=None, save=False)\n",
    "    #     rgb_images_thumbs_list.append(rgb_images_thumbs.tolist())\n",
    "    \n",
    "    # rgb_images_thumbs_list = torch.Tensor(rgb_images_thumbs_list, requires_grad=True).to(device=device)\n",
    "    # return rgb_images_thumbs_list\n",
    "    pass\n",
    "\n",
    "def evalmodel(model, testloader, lossModel_latent, epoch):\n",
    "    prob = 0.1\n",
    "    model.eval()\n",
    "    test_loss_latent = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(testloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output_latent = model(data)\n",
    "            # loss_cnn = lossModel_latent(output_latent,target)\n",
    "            loss_latent = lossModel_latent(output_latent[:, :256],target[:, :256])\n",
    "            test_loss_latent += loss_latent\n",
    "\n",
    "            if random.random() < prob:\n",
    "                seeIdx = random.randint(0, batch_size-1)\n",
    "                # para\n",
    "                # concate = torch.concat([target[:, :256], output_latent[:, 256:]], dim=1)\n",
    "                # sample_z\n",
    "                concate = torch.concat([output_latent[:, :256], target[:, 256:]], dim=1)\n",
    "                \n",
    "                # normal\n",
    "                # generateImage(output_latent, g_ema, seeIdx, f'{epoch}_{batch_idx}_output')\n",
    "                # para / sample_z\n",
    "                # print(f'concate.shape: {concate.shape}')\n",
    "                generateImage(concate, g_ema, seeIdx, f'{epoch}_{batch_idx * batch_size + seeIdx +train_test_split_point}_output')\n",
    "                # generateImage(target, g_ema, seeIdx, f'{epoch}_{batch_idx * batch_size + seeIdx + train_test_split_point}_generateTarget')\n",
    "                save_image([data[seeIdx]], f'./result/images/{epoch}_{batch_idx * batch_size + seeIdx + train_test_split_point}_realTarget.png')\n",
    "                # print(f'real: {data[seeIdx]}')\n",
    "                \n",
    "\n",
    "    test_loss_latent /= len(testloader.dataset)\n",
    "    # print(f'len(testloader.dataset): {len(testloader.dataset)}')\n",
    "    return test_loss_latent\n",
    "\n",
    "def train(model, optimizer, dataloader_train, testloader, lossModel_latent, lossModel_image, total_epoch, scheduler):\n",
    "    useImageLoss = False\n",
    "    \n",
    "    # 步驟5. CNN模型開始訓練\n",
    "    loss_train_list=[]\n",
    "    loss_test_list=[]\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        scheduler.step()\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        train_loss_latent = 0\n",
    "        train_loss_image = 0\n",
    "        # for batch_idx, (data, target) in enumerate(dataloader_train):\n",
    "        for (data, target) in tqdm(dataloader_train):\n",
    "        # for (data, target) in dataloader_train:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # optimizer.zero_grad()\n",
    "            output_cnn = model(data)\n",
    "            \n",
    "            # normal\n",
    "            # loss_cnn = loss_latent(output_cnn,target) * loss_scaling\n",
    "            # paras\n",
    "            # loss_cnn = loss_latent(output_cnn[:, 256:],target[:, 256:])# * loss_scaling # train sample_z\n",
    "            # sample_z\n",
    "            loss_latent = lossModel_latent(output_cnn[:, :256],target[:, :256])# * loss_scaling # train sample_z\n",
    "            # loss_latent2 = lossModel_latent(output_cnn,target)# * loss_scaling # train sample_z\n",
    "            # print(loss_latent)\n",
    "            # print(loss_latent2)\n",
    "            # loss_latent = lossModel_latent(output_cnn,target)# * loss_scaling # train sample_z\n",
    "            # concate = torch.concat([output_cnn[:, :256], target[:, 256:]], dim=1)\n",
    "\n",
    "            # output_image = generateImageBatch(latent=output_cnn[:, :256], para=target[:, 256:], g_ema=g_ema)\n",
    "            # loss_image = lossModel_image(output_image, data)\n",
    "            # total_loss = loss_image + loss_latent\n",
    "            # train_loss_latent += loss_latent\n",
    "            # train_loss_image += loss_image\n",
    "\n",
    "            # total_loss = loss_latent\n",
    "\n",
    "            # print(loss_cnn)\n",
    "            # print(loss_image)\n",
    "            \n",
    "\n",
    "            # loss_latent.backward()\n",
    "            # loss_image.backward()\n",
    "            # total_loss.backward()\n",
    "            optimizer.zero_grad()\n",
    "            loss_latent.backward()\n",
    "            # print(loss_latent.item())\n",
    "            optimizer.step()\n",
    "            train_loss_latent += loss_latent.item()\n",
    "            # print(loss_latent.item())\n",
    "            # print(train_loss_latent)\n",
    "\n",
    "        # print(train_loss_latent)\n",
    "        train_loss_latent /= len(dataloader_train.dataset)\n",
    "        # print(f'len(dataloader_train.dataset): {len(dataloader_train.dataset)}')\n",
    "        if useImageLoss:\n",
    "            train_loss_image /= len(dataloader_train.dataset)\n",
    "        \n",
    "    \n",
    "        if epoch % 10 == 0:\n",
    "            test_loss = evalmodel(model, testloader, lossModel_latent, epoch)\n",
    "            \n",
    "            # seeIdx = 0\n",
    "            # # generateImage(output_cnn, g_ema, seeIdx, f'{epoch}_output')\n",
    "            # concate = torch.concat([target[:, :256], output_cnn[:, 256:]], dim=1)\n",
    "            # print(f'concate.shape: {concate.shape}')\n",
    "            # generateImage(concate, g_ema, seeIdx, f'{epoch}_output')\n",
    "            # generateImage(target, g_ema, seeIdx, f'{epoch}_target')\n",
    "            \n",
    "            # loss_train_list.append(total_loss)\n",
    "            loss_test_list.append(test_loss)\n",
    "            print('learning rate:{}'.format(scheduler.get_last_lr()[0]))\n",
    "            print(F'CNN[epoch: [{epoch+1}/{total_epoch}], Average loss latent/image (Train):{train_loss_latent}/{train_loss_image},  Average loss latent (test):{test_loss}')\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save(model.state_dict(), f'./result/models/{epoch}.ckpt')\n",
    "\n",
    "    print(F'CNN[epoch: [{epoch+1}/{total_epoch}], Average loss latent/image (Train):{train_loss_latent}/{train_loss_image},  Average loss latent (test):{test_loss}')\n",
    "    print('training done.')\n",
    "    \n",
    "    return loss_train_list, loss_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Training ... \n",
      "resNetEncoder latent only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:14<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concate.shape: torch.Size([64, 268])\n",
      "concate.shape: torch.Size([64, 268])\n",
      "concate.shape: torch.Size([64, 268])\n",
      "concate.shape: torch.Size([64, 268])\n",
      "learning rate:0.001\n",
      "CNN[epoch: [1/100], Average loss latent/image (Train):0.4979997551739216/0,  Average loss latent (test):0.05009942129254341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:12<00:00, 10.38it/s]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.54it/s]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.52it/s]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.50it/s]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.47it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.41it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.39it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.31it/s]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.50it/s]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concate.shape: torch.Size([64, 268])\n",
      "concate.shape: torch.Size([64, 268])\n",
      "concate.shape: torch.Size([16, 268])\n",
      "learning rate:0.001\n",
      "CNN[epoch: [11/100], Average loss latent/image (Train):0.015947767414152623/0,  Average loss latent (test):0.02273315191268921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:11<00:00, 10.55it/s]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.50it/s]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.46it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.13it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.18it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.24it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.32it/s]\n",
      "100%|██████████| 125/125 [00:11<00:00, 10.45it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.36it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concate.shape: torch.Size([64, 268])\n",
      "learning rate:0.001\n",
      "CNN[epoch: [21/100], Average loss latent/image (Train):0.0086956601254642/0,  Average loss latent (test):0.02189939096570015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:12<00:00, 10.22it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.35it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.29it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.28it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.29it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.34it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.28it/s]\n",
      "100%|██████████| 125/125 [00:12<00:00, 10.30it/s]\n",
      " 78%|███████▊  | 98/125 [00:09<00:02, 10.35it/s]"
     ]
    }
   ],
   "source": [
    "print('*'*50)\n",
    "print('Training ... ')\n",
    "print(\"resNetEncoder latent only\")\n",
    "loss_train_list, loss_test_list = train(encoder, optimizer, trainData_loader, testData_loader, lossModel_latent, lossModel_image, total_epoch=epochs, scheduler=scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loss_train_list)):\n",
    "    loss_train_list[i] = loss_train_list[i].cpu().detach().numpy()\n",
    "\n",
    "for i in range(len(loss_test_list)):\n",
    "    loss_test_list[i] = loss_test_list[i].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_train_list, color='red', label='train loss')\n",
    "plt.plot(loss_test_list, color='blue', label='test loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'./result/loss/lossBoth.png')\n",
    "plt.cla()\n",
    "\n",
    "plt.plot(loss_train_list, color='red', label='train loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'./result/loss/lossTrain.png')\n",
    "plt.cla()\n",
    "\n",
    "plt.plot(loss_test_list, color='blue', label='test loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'./result/loss/lossTest.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
