{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as FT\n",
    "from torchvision import utils\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import subprocess\n",
    "# from generate_shapes_and_images import generate, generateImage\n",
    "from model import Generator\n",
    "\n",
    "# from myService.myModel import *\n",
    "# from myService.myDataset import MyDataset\n",
    "from myService.myUtils import my_collate\n",
    "from myService.getImages import GetImages\n",
    "from options import BaseOptions\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision.utils import save_image\n",
    "from swinModels.swin_transformer import SwinTransformer\n",
    "\n",
    "random.seed(datetime.now().timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= 100\n",
    "batch_size= 64\n",
    "\n",
    "truncation_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_identities = 1\n",
    "\n",
    "opt = BaseOptions().parse()\n",
    "opt.camera.uniform = True\n",
    "opt.model.is_test = True\n",
    "opt.model.freeze_renderer = False\n",
    "opt.rendering.offset_sampling = True\n",
    "opt.rendering.static_viewdirs = True\n",
    "opt.rendering.force_background = True\n",
    "opt.rendering.perturb = 0\n",
    "opt.inference.renderer_output_size = opt.model.renderer_spatial_output_dim = 64\n",
    "opt.inference.style_dim = opt.model.style_dim\n",
    "opt.inference.project_noise = opt.model.project_noise\n",
    "\n",
    "# User options\n",
    "model_type = 'ffhq' # Whether to load the FFHQ or AFHQ model\n",
    "opt.inference.no_surface_renderings = True # When true, only RGB images will be created\n",
    "opt.inference.fixed_camera_angles = False # When true, each identity will be rendered from a specific set of 13 viewpoints. Otherwise, random views are generated\n",
    "opt.inference.identities = inference_identities # Number of identities to generate\n",
    "opt.inference.num_views_per_id = 1 # Number of viewpoints generated per identity. This option is ignored if self.opt.inference.fixed_camera_angles is true.\n",
    "\n",
    "opt.model.size = 1024\n",
    "opt.experiment.expname = 'ffhq1024x1024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = 1\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() & use_cuda) else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    '''\n",
    "    load the dataset\n",
    "    '''\n",
    "    def __init__(self, startIdx, endIdx, transform = None):\n",
    "        self.startIdx = startIdx\n",
    "        self.endIdx = endIdx\n",
    "        \n",
    "        camera_json_path = './prepareDataset/json/camera_paras.json'\n",
    "        # z_json_path = './prepareDataset/json/sample_z_actual_used.json'\n",
    "        z_json_path = './prepareDataset/json/sample_z.json'\n",
    "        \n",
    "        default_transform = transforms.Compose([\n",
    "            # transforms.Resize((28,28)),\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "        with open(camera_json_path) as jsonFile:\n",
    "            camera_paras = json.load(jsonFile)\n",
    "\n",
    "        with open(z_json_path) as jsonFile:\n",
    "            sample_z = json.load(jsonFile)\n",
    "        \n",
    "        self.camera_paras = camera_paras[startIdx: endIdx]\n",
    "        self.sample_z = sample_z[startIdx: endIdx]\n",
    "        # print(f'self.sample_z.shape: {np.array(self.sample_z).shape}')\n",
    "\n",
    "        if transform == None:\n",
    "            self.transform = default_transform\n",
    "        else:\n",
    "            self.transform = transform\n",
    "        print('number of total data:{}'.format(len(self.camera_paras)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.camera_paras)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        :param idx: Index of the image file\n",
    "        :return: returns the image and corresponding label file.\n",
    "        '''\n",
    "        # read image with PIL module\n",
    "        image_name = './prepareDataset/thumbs/' + str(self.startIdx + idx).rjust(7, \"0\") + \".png\"\n",
    "        image = Image.open(image_name, mode='r')\n",
    "        image = image.convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # len: 256\n",
    "        sample_z = self.sample_z[idx][0] \n",
    "        # print(np.array(sample_z).shape)\n",
    "\n",
    "        # len: 12\n",
    "        # camera_paras = np.array(self.camera_paras[self.startIdx + idx][\"sample_cam_extrinsics\"][0]).flatten().tolist() + self.camera_paras[self.startIdx + idx][\"sample_locations\"][0]\n",
    "        camera_paras = np.array(self.camera_paras[idx][\"sample_cam_extrinsics\"][0]).flatten().tolist()\n",
    "\n",
    "        # len: 256 + 12 = 268\n",
    "        target = sample_z + camera_paras\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "        return (image, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_point = 8000\n",
    "trainData = MyDataset(startIdx=0, endIdx=train_test_split_point, transform=None)\n",
    "trainData_loader = DataLoader(trainData, batch_size=batch_size, num_workers=0,  collate_fn = my_collate, shuffle=True)\n",
    "\n",
    "testData = MyDataset(startIdx=train_test_split_point, endIdx=10001, transform=None)\n",
    "testData_loader = DataLoader(testData, batch_size=batch_size, num_workers=0,  collate_fn = my_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in trainData_loader:\n",
    "    print(data[0].shape)\n",
    "    print(data[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvM(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        super(ConvM, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "            norm_layer(out_planes),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, n_class=16):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            ConvM(3, 32, 5, 2),\n",
    "            ConvM(32, 32, 5, 2),\n",
    "            ConvM(32, 32, 3, 1),\n",
    "            ConvM(32, 32, 3, 1),\n",
    "        )        \n",
    "        self.fc1 = nn.Linear(32, 256)\n",
    "        self.fc2 = nn.Linear(256, 1000)\n",
    "        self.fc3 = nn.Linear(1000, n_class)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, 1).reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block with instance normalization.\"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.main(x)\n",
    "class ResnetEncoder(nn.Module):\n",
    "    # 202005251539 attr dim\n",
    "    def __init__(self, input_nc=3, output_nc=3, n_blocks=3): \n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetEncoder, self).__init__()\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        ngf = 64\n",
    "        padding_type ='reflect'\n",
    "        norm_layer = nn.InstanceNorm2d\n",
    "        use_bias = False\n",
    "        \n",
    "        model = [nn.Conv2d(input_nc , ngf, kernel_size=7, padding=3,\n",
    "                           bias=use_bias),\n",
    "                 norm_layer(ngf, affine=True, track_running_stats=True),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=4,\n",
    "                                stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2, affine=True, track_running_stats=True),\n",
    "                      nn.ReLU(True)]\n",
    "        mult = 2**n_downsampling\n",
    "        \n",
    "        for i in range(n_blocks):\n",
    "            model += [ResidualBlock(dim_in=ngf * mult, dim_out=ngf * mult)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        # 65536\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, 268)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "convNet = ConvNet(n_class=268).to(device)\n",
    "resnetEncoder = ResnetEncoder().to(device)\n",
    "swin_transformer = SwinTransformer(img_size=64, window_size=4, num_classes=268).to(device)\n",
    "\n",
    "encoder = swin_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model\n",
    "# epoch = 10\n",
    "# resnetEncoder_path = f\"./result/models/{epoch}.ckpt\"\n",
    "# resnetEncoder.load_state_dict(torch.load(resnetEncoder_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = torch.nn.CrossEntropyLoss().to(device)\n",
    "lossModel_image = torch.nn.MSELoss().to(device)\n",
    "# lossModel_image = torch.nn.CrossEntropyLoss().to(device)\n",
    "# lossModel_image = torch.nn.L1Loss().to(device)\n",
    "lossModel_latent = torch.nn.MSELoss().to(device)\n",
    "# lossModel_latent = torch.nn.CrossEntropyLoss().to(device)\n",
    "# lossModel_latent = torch.nn.L1Loss().to(device)\n",
    "# loss = torch.nn.SmoothL1Loss().to(device)\n",
    "# optimizer = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=0.001, betas=[0.5, 0.999])\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20,30,40,50,60,70,80,90], gamma=0.7)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get g_ema\n",
    "model_path = 'ffhq1024x1024.pt'\n",
    "checkpoint_path = os.path.join('full_models', model_path)\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "g_ema = Generator(model_opt=opt.model, renderer_opt=opt.rendering, full_pipeline=True).to(device)\n",
    "\n",
    "pretrained_weights_dict = checkpoint[\"g_ema\"]\n",
    "# pretrained_weights_dict = checkpoint[\"g\"]\n",
    "model_dict = g_ema.state_dict()\n",
    "for k, v in pretrained_weights_dict.items():\n",
    "    if v.size() == model_dict[k].size():\n",
    "        model_dict[k] = v\n",
    "\n",
    "g_ema.load_state_dict(model_dict)\n",
    "g_ema.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./prepareDataset/json/mean_latent.json\", 'r') as jsonFile:\n",
    "    mean_latent = json.load(jsonFile)\n",
    "for i in range(len(mean_latent)):\n",
    "    mean_latent[i] = torch.Tensor(mean_latent[i]).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateImage(latent, g_ema, seeIdx, fileName, save=True):\n",
    "    # all are tensor\n",
    "    latent = latent[seeIdx]\n",
    "    \n",
    "    sample_z = latent[0:256].reshape(1,256)\n",
    "    sample_cam_extrinsics = torch.Tensor([[\n",
    "        latent[256:260].tolist(),\n",
    "        latent[260:264].tolist(),\n",
    "        latent[264:268].tolist(),\n",
    "    ]]).to(device)\n",
    "    sample_focals = torch.Tensor([[\n",
    "        [\n",
    "            304.45965576171875\n",
    "        ]\n",
    "    ]]).to(device)\n",
    "    sample_near = torch.Tensor([[\n",
    "        [\n",
    "            0.8799999952316284\n",
    "        ]\n",
    "    ]]).to(device)\n",
    "    sample_far = torch.Tensor([[\n",
    "        [\n",
    "            1.1200000047683716\n",
    "        ]\n",
    "    ]]).to(device)\n",
    "    out = g_ema([sample_z],\n",
    "            sample_cam_extrinsics,\n",
    "            sample_focals,\n",
    "            sample_near,\n",
    "            sample_far,\n",
    "            truncation=truncation_ratio,\n",
    "            truncation_latent=mean_latent)\n",
    "    opt.renderer_output_size = 64\n",
    "    rgb_images_thumbs = torch.Tensor(0, 3, opt.renderer_output_size, opt.renderer_output_size)\n",
    "    rgb_images_thumbs = torch.cat([rgb_images_thumbs, out[1].cpu()], 0)\n",
    "\n",
    "    rgb_images_thumbs = rgb_images_thumbs.reshape(3,64,64)\n",
    "    # print(f'generate: {rgb_images_thumbs}')\n",
    "    if save:\n",
    "        utils.save_image(rgb_images_thumbs,\n",
    "            # os.path.join(prepareDatasetPath, 'thumbs',f'{str(i).zfill(7)}.png'),\n",
    "            f'./result/images/{fileName}.png',\n",
    "            nrow=1,\n",
    "            normalize=True,\n",
    "            padding=0,\n",
    "            value_range=(-1, 1),)\n",
    "    # save_image([rgb_images_thumbs], f'./result/images/{fileName}.png')\n",
    "    \n",
    "    # image = Image.fromarray(rgb_images_thumbs.permute(1, 2, 0))\n",
    "    # image.save(f'{fileName}.png', format='PNG')\n",
    "    # %matplotlib inline\n",
    "    # plt.imshow(  rgb_images_thumbs.permute(1, 2, 0)  )\n",
    "    # plt.imshow(  rgb_images_thumbs  )\n",
    "    return rgb_images_thumbs\n",
    "\n",
    "def generateImageBatch(latent, para, g_ema):\n",
    "    sample_cam_extrinsics = para[:, :12].reshape(-1,3,4)\n",
    "    sample_focals = torch.Tensor([[\n",
    "        [\n",
    "            304.45965576171875\n",
    "        ]\n",
    "    ]]*batch_size).to(device)\n",
    "    sample_near = torch.Tensor([[\n",
    "        [\n",
    "            0.8799999952316284\n",
    "        ]\n",
    "    ]]*batch_size).to(device)\n",
    "    sample_far = torch.Tensor([[\n",
    "        [\n",
    "            1.1200000047683716\n",
    "        ]\n",
    "    ]]*batch_size).to(device)\n",
    "    \n",
    "    chunk = 2\n",
    "    \n",
    "    thumb_rgb = torch.Tensor(0, 3, 64, 64).to(device)\n",
    "    thumb_rgb.requires_grad = True\n",
    "\n",
    "    for j in range(0, batch_size, chunk):\n",
    "        out = g_ema([latent[j:j+chunk]],\n",
    "                    sample_cam_extrinsics[j:j+chunk],\n",
    "                    sample_focals[j:j+chunk],\n",
    "                    sample_near[j:j+chunk],\n",
    "                    sample_far[j:j+chunk],\n",
    "                    truncation=truncation_ratio,\n",
    "                    truncation_latent=mean_latent)\n",
    "\n",
    "        # rgb_images = torch.cat([rgb_images, out[0].cpu()], 0)\n",
    "        thumb_rgb = torch.cat([thumb_rgb, out[1]], 0)\n",
    "\n",
    "        del out\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return thumb_rgb\n",
    "    \n",
    "    # all are tensor\n",
    "    # rgb_images_thumbs_list = []\n",
    "    # for i in range(len(latent)):\n",
    "    #     rgb_images_thumbs = generateImage(latent=latent, g_ema=g_ema, seeIdx=i, fileName=None, save=False)\n",
    "    #     rgb_images_thumbs_list.append(rgb_images_thumbs.tolist())\n",
    "    \n",
    "    # rgb_images_thumbs_list = torch.Tensor(rgb_images_thumbs_list, requires_grad=True).to(device=device)\n",
    "    # return rgb_images_thumbs_list\n",
    "    pass\n",
    "\n",
    "def evalmodel(model, testloader, lossModel_latent, epoch):  \n",
    "    prob = 0.1\n",
    "    model.eval()\n",
    "    # test_loss_latent = 0\n",
    "    test_loss_para = 0\n",
    "    # test_loss_image = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(testloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output_latent = model(data)\n",
    "\n",
    "            # loss_latent = lossModel_latent(output_latent[:, :256],target[:, :256])\n",
    "            loss_para = lossModel_latent(output_latent[:, 256:],target[:, 256:])\n",
    "            test_loss_para += loss_para\n",
    "            # test_loss_latent += loss_latent\n",
    "\n",
    "            # output_image = generateImageBatch(latent=output_latent[:, :256], para=target[:, 256:], g_ema=g_ema)\n",
    "            # loss_image = lossModel_image(output_image, data)\n",
    "            # test_loss_image += loss_image\n",
    "\n",
    "            if random.random() < prob:\n",
    "                # concate = torch.concat([output_latent[:, :256], target[:, 256:]], dim=1)\n",
    "                concate = torch.concat([target[:, :256], output_latent[:, 256:]], dim=1)\n",
    "                seeIdx = random.randint(0, concate.shape[0]-1)\n",
    "                \n",
    "                generateImage(concate, g_ema, seeIdx, f'{epoch}_{batch_idx * batch_size + seeIdx +train_test_split_point}_output')\n",
    "                save_image([data[seeIdx]], f'./result/images/{epoch}_{batch_idx * batch_size + seeIdx + train_test_split_point}_realTarget.png')\n",
    "                \n",
    "\n",
    "    # test_loss_latent /= len(testloader.dataset)\n",
    "    # test_loss_image /= len(testloader.dataset)\n",
    "    test_loss_para /= len(testloader.dataset)\n",
    "    # return test_loss_latent\n",
    "    return test_loss_para\n",
    "\n",
    "def train(model, optimizer, dataloader_train, testloader, lossModel_latent, lossModel_image, total_epoch, scheduler):\n",
    "    useImageLoss = False\n",
    "    checkpoint = 5\n",
    "    \n",
    "    # 步驟5. CNN模型開始訓練\n",
    "    loss_train_list=[]\n",
    "    loss_test_list=[]\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        scheduler.step()\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        # train_loss_latent = 0\n",
    "        train_loss_para = 0\n",
    "        # train_loss_image = 0\n",
    "        for (data, target) in tqdm(dataloader_train):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output_latent = model(data)\n",
    "            \n",
    "            # loss_latent = lossModel_latent(output_latent[:, :256],target[:, :256])\n",
    "            loss_para = lossModel_latent(output_latent[:, 256:],target[:, 256:])\n",
    "            \n",
    "            # output_image = generateImageBatch(latent=output_latent[:, :256], para=target[:, 256:], g_ema=g_ema)\n",
    "            # loss_image = lossModel_image(output_image, data)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # loss_latent.backward()\n",
    "            loss_para.backward()\n",
    "            # loss_image.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            # train_loss_latent += loss_latent.item()\n",
    "            train_loss_para += loss_para.item()\n",
    "            # train_loss_image += loss_image.item()\n",
    "\n",
    "        # train_loss_latent /= len(dataloader_train.dataset)\n",
    "        train_loss_para /= len(dataloader_train.dataset)\n",
    "        # train_loss_image /= len(dataloader_train.dataset)\n",
    "        \n",
    "    \n",
    "        if epoch % checkpoint == 0:\n",
    "            test_loss = evalmodel(model, testloader, lossModel_latent, epoch)\n",
    "            \n",
    "            # loss_train_list.append(total_loss)\n",
    "            loss_test_list.append(test_loss)\n",
    "            print('learning rate:{}'.format(scheduler.get_last_lr()[0]))\n",
    "            print(F'CNN[epoch: [{epoch+1}/{total_epoch}], Average loss para (Train):{train_loss_para},  Average loss latent (test):{test_loss}')\n",
    "\n",
    "        if epoch % checkpoint == 0:\n",
    "            torch.save(model.state_dict(), f'./result/models/{epoch}.ckpt')\n",
    "\n",
    "    print(F'CNN[epoch: [{epoch+1}/{total_epoch}], Average loss para (Train):{train_loss_para},  Average loss latent (test):{test_loss}')\n",
    "    print('training done.')\n",
    "    \n",
    "    return loss_train_list, loss_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training ... ')\n",
    "loss_train_list, loss_test_list = train(encoder, optimizer, trainData_loader, testData_loader, lossModel_latent, lossModel_image, total_epoch=epochs, scheduler=scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loss_train_list)):\n",
    "    loss_train_list[i] = loss_train_list[i].cpu().detach().numpy()\n",
    "\n",
    "for i in range(len(loss_test_list)):\n",
    "    loss_test_list[i] = loss_test_list[i].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_train_list, color='red', label='train loss')\n",
    "plt.plot(loss_test_list, color='blue', label='test loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'./result/loss/lossBoth.png')\n",
    "plt.cla()\n",
    "\n",
    "plt.plot(loss_train_list, color='red', label='train loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'./result/loss/lossTrain.png')\n",
    "plt.cla()\n",
    "\n",
    "plt.plot(loss_test_list, color='blue', label='test loss')\n",
    "plt.legend()\n",
    "plt.savefig(f'./result/loss/lossTest.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
